require(polr)
install.packages("polr")
require(MASS)
dat <- read.dta("https://stats.idre.ucla.edu/stat/data/ologit.dta")
require(tidyverse)
require(foreign)
dat <- read.dta("https://stats.idre.ucla.edu/stat/data/ologit.dta")
dat
str(dat)
m <- polr(apply ~ pared + public + gpa,
data = dat,
Hess = TRUE)
summary(m)
#store table
(ctable <- coef(summary(m)))
#calculate and store p values
p <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
## combine table
(ctable <- cbind(ctable, "p value" = p))
summary(m)
summary(m)
m$df.residual
m$
#store table
(ctable <- coef(summary(m)))
#store table
(ctable <- coef(summary(m)))
ctable
ctable[, "t value"]
pt(q=-.77, df=15, lower.tail=FALSE)
pt(q=.77, df=15, lower.tail=FALSE)
?pt
pt(q=-.77, df=15, lower.tail=TRUE)
pt(q=.77, df=15, lower.tail=FALSE)
pt(q=abs(ctable[, "t value"]), df=15, lower.tail=FALSE) * 2
pt(q=abs(ctable[, "t value"]), df=m$df.residual, lower.tail=FALSE) * 2
#calculate and store p values
p <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
p
#profiled CIs
(ci <- confint(m))
#normal CIs
confint.default(m)
#odds ratios
exp(coef(m))
## OR and CI
exp(cbind(OR = coef(m), ci))
#different package
require(ordinal)
m2 <- clm(apply ~ pared + public + gpa,
data = dat)
summary(m2)
p
#compute p values using the t distribution
pt(q=abs(ctable[, "t value"]), df=m$df.residual, lower.tail=FALSE) * 2
?clm
summary(m2)
summary(m)
setwd("~/GitHub/sentence_embeddings_for_surveys")
# analysis for probing tasks
dataset = read_csv('merged.csv')
library(tidyverse)
# analysis for probing tasks
dataset = read_csv('merged.csv')
dataset
dataset %>%
distinct(length_binned)
dataset %>%
filter(length_binned == '0-10')
dataset %>%
filter(length_binned == '0-10') %>%
select(row_id) %>%
sample_n(100)
set.seed(123)
dataset %>%
filter(length_binned == '0-10') %>%
select(row_id) %>%
sample_n(100)
dataset %>%
filter(length_binned == '0-10') %>%
select(row_id) %>%
sample_n(100) %>%
write_csv('sample_ids_concrete_concept.csv')
set.seed(123)
dataset %>%
filter(length_binned == '0-10') %>%
select(row_id) %>%
sample_n(100) %>%
write_csv('sample_ids_concrete_concept.csv')
library(readxl)
questions_df = read_excel('Synthetic_Questions_Controlled_Variants_20210614.xlsx')
questions_df = read_excel('./data/synthetic/Synthetic_Questions_Controlled_Variants_20210614.xlsx')
questions_df
set.seed(123)
dataset %>%
filter(length_binned == '0-10') %>%
select(row_id, concrete_concept)
set.seed(123)
dataset %>%
filter(length_binned == '0-10') %>%
select(row_id, concrete_concept) %>%
sample_n(100) %>%
write_csv('sample_ids_concrete_concept.csv')
set.seed(123)
dataset %>%
filter(length_binned == '0-10') %>%
select(row_id, concrete_concept) %>%
sample_n(100)
questions_df
questions_df = read_excel('./data/synthetic/Synthetic_Questions_Controlled_Variants_20210614.xlsx') %>%
select(row_id, rfa)
dataset %>%
filter(length_binned == '0-10') %>%
select(row_id, concrete_concept) %>%
sample_n(100) %>%
left_join(questions_df)
set.seed(123)
dataset %>%
filter(length_binned == '0-10') %>%
select(row_id, concrete_concept) %>%
sample_n(100) %>%
left_join(questions_df)
set.seed(123)
dataset %>%
filter(length_binned == '0-10') %>%
select(row_id, concrete_concept) %>%
sample_n(100) %>%
left_join(questions_df) %>%
write_csv('sample_ids_concrete_concept.csv')
read_excel('./data/synthetic/Synthetic_Questions_Controlled_Variants_20210614.xlsx')
dataset
features_df = read_excel('./data/synthetic/Synthetic_Questions_Controlled_20210614.xlsx')
features_df = read_excel('./data/synthetic/Synthetic_Questions_Controlled_20210611.xlsx')
features_df
setwd("~/GitHub/sentence_embeddings_for_surveys")
library(tidyverse)
# analysis for probing tasks
dataset = read_csv('merged.csv')
dataset
dataset %>%
filter(length_binned == "15-25") %>%
ggplot(aes(x = length_binned)) +
geom_bar() +
facet_wrap(~basic_concept)
# probe basic concept:
dataset %>%
ggplot(aes(x = concrete_concept)) +
geom_bar() +
facet_wrap(~concrete_concept)
# probe basic concept:
dataset %>%
ggplot(aes(x = length_binned)) +
geom_bar() +
facet_wrap(~concrete_concept)
library(tidyverse)
results <- read_csv('prediction_results.csv')
results
results %>%
group_by(model, feature) %>%
summarise(mean_baseline = mean(baseline_average_response_within),
sd_baseline = sd(baseline_average_response_within),
mean_MSE = mean(MSE),
sd_MSE = sd(MSE)
)
results %>%
group_by(model, feature) %>%
summarise(mean_baseline = mean(baseline_average_response_within),
sd_baseline = sd(baseline_average_response_within),
mean_MSE = mean(MSE),
sd_MSE = sd(MSE)
) %>%
print(n = Inf)
results
results %>%
replace_na(list(Pearson_R = 0))
results %>%
replace_na(list(Pearson_R = 0)) %>%
group_by(model, feature) %>%
summarise(mean_r = mean(Pearson_R)
)
results %>%
replace_na(list(Pearson_R = 0)) %>%
group_by(model, feature) %>%
summarise(mean_r = mean(Pearson_R)) %>%
print(n = Inf)
results %>%
filter(is.na(Pearson_R))
results %>%
filter(is.na(Pearson_R))
results %>%
filter(is.na(Pearson_R)) %>%
select(Pearson_R)
results %>%
replace_na(list(Pearson_R = 0)) %>%
filter(is.na(Pearson_R)) %>%
select(Pearson_R)
results %>%
replace_na(list(Pearson_R = 0)) %>%
group_by(model, feature) %>%
summarise(mean_r = mean(Pearson_R)) %>%
print(n = Inf)
